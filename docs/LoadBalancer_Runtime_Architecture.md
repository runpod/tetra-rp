# Load-Balanced Endpoint Runtime Architecture

## Overview

This document explains what happens after a load-balanced endpoint is deployed on RunPod and is actively running. It covers the deployment architecture, request flows, and execution patterns for both direct HTTP requests and @remote function calls.

## Deployment Architecture

### Container Image and Startup

When you deploy a `LoadBalancerSlsResource` endpoint with `flash build` and `flash deploy`:

```mermaid
graph TD
    A["User Code"] -->|flash build| B["Generate handler_service.py"]
    B -->|FastAPI App| C["handler_service.py"]
    C -->|flash deploy| D["Push to RunPod"]
    D -->|Create Container| E["RunPod Container<br/>tetra-rp-lb image"]
    E --> F["FastAPI Server<br/>uvicorn on port 8000"]
    F --> G["Load your handler"]
    G --> H["Endpoint Ready"]

    style A fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style B fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style C fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style D fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style E fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style F fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style G fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style H fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff
```

**Important:** `endpoint_url` is auto-generated by RunPod after deployment
- Cannot be specified by users
- Generated as: `https://<runpod-base>/<endpoint-id>`
- Automatically populated in the resource after `deploy()` completes
- Available via `resource.endpoint_url` property (read-only)

### What Gets Deployed

The generated handler file contains:

```python
# handler_service.py (auto-generated)
from fastapi import FastAPI
from tetra_rp.runtime.lb_handler import create_lb_handler

# User functions imported
from api.endpoints import process_data
from api.health import health_check

# Route registry
ROUTE_REGISTRY = {
    ("POST", "/api/process"): process_data,
    ("GET", "/api/health"): health_check,
}

# FastAPI app created
app = create_lb_handler(ROUTE_REGISTRY)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Container Setup:**
- Base image: `runpod/tetra-rp-lb:latest` (contains FastAPI, uvicorn, dependencies)
- Entrypoint: Runs `python handler_service.py`
- Port: 8000 (internal)
- RunPod exposes this via HTTPS endpoint URL
- Health check: Polls `/ping` endpoint every 30 seconds with 15 second timeout per check
- All HTTP requests to the endpoint include authentication via `RUNPOD_API_KEY` environment variable (if set)

### Deployment Lifecycle

```mermaid
graph TD
    A["LoadBalancerSlsResource created"] -->|flash build| B["Generate handler file"]
    B -->|flash deploy| C["Push to RunPod"]
    C --> D["RunPod creates container"]
    D --> E["Container starts uvicorn"]
    E --> F["FastAPI app loads"]
    F --> G["Import user functions"]
    G --> H["Register routes"]
    H --> I["Endpoint ready"]
    I --> J["Health checks pass"]
    J --> K["Endpoint active"]
```

## Request Flow

### Direct HTTP Request (User Routes)

When a client makes an HTTP request to your deployed endpoint:

```mermaid
sequenceDiagram
    participant Client
    participant RunPod as RunPod Router
    participant Container as Endpoint Container
    participant FastAPI
    participant UserFunc as User Function

    Client->>RunPod: HTTPS POST /api/process
    RunPod->>Container: Forward to port 8000
    Container->>FastAPI: HTTP POST /api/process
    FastAPI->>FastAPI: Match (POST, /api/process)<br/>in ROUTE_REGISTRY
    FastAPI->>UserFunc: Call process_data(x=5, y=3)
    UserFunc->>UserFunc: Execute function code
    UserFunc-->>FastAPI: Return {"result": 8}
    FastAPI->>FastAPI: Serialize to JSON
    FastAPI-->>Container: HTTP 200 response
    Container-->>RunPod: Response body
    RunPod-->>Client: HTTPS response
```

**Example Flow:**

```python
# User code
@remote(api, method="POST", path="/api/process")
async def process_data(x: int, y: int):
    return {"result": x + y}

# Client request
POST https://my-endpoint.runpod.ai/api/process
Content-Type: application/json
{"x": 5, "y": 3}

# On RunPod:
# 1. Request arrives at container port 8000
# 2. FastAPI receives POST /api/process
# 3. FastAPI parses JSON body: {"x": 5, "y": 3}
# 4. FastAPI calls process_data(x=5, y=3)
# 5. Function executes: returns {"result": 8}
# 6. FastAPI serializes response
# 7. Returns HTTP 200 with body {"result": 8}
# 8. RunPod wraps in HTTPS response
# 9. Client receives response
```

### @remote Function Call (Framework Endpoint)

When you call an `@remote` decorated function from your local code:

```mermaid
sequenceDiagram
    participant Local as Local Code
    participant Stub as LoadBalancerSlsStub
    participant RunPod as RunPod Router
    participant Container as Endpoint Container
    participant Execute as /execute Handler

    Local->>Stub: await process_data(5, 3)
    Stub->>Stub: Extract function source code<br/>via AST inspection
    Stub->>Stub: Serialize args with cloudpickle<br/>+ base64 encode
    Stub->>RunPod: POST /execute
    RunPod->>Container: Forward to port 8000
    Container->>Execute: HTTP POST /execute
    Execute->>Execute: Parse JSON body
    Execute->>Execute: Deserialize arguments<br/>(base64 decode + cloudpickle loads)
    Execute->>Execute: Extract function code string
    Execute->>Execute: exec(code) in isolated namespace
    Execute->>Execute: Call func(5, 3)
    Execute->>Execute: Get result: {"result": 8}
    Execute->>Execute: Serialize result with cloudpickle<br/>+ base64 encode
    Execute-->>Container: HTTP 200 {success: true, result: base64}
    Container-->>RunPod: Response body
    RunPod-->>Stub: Response body
    Stub->>Stub: Deserialize result<br/>(base64 decode + cloudpickle loads)
    Stub-->>Local: Return {"result": 8}
```

**Example Flow:**

```python
# Local code - after deployment
api = LoadBalancerSlsResource(name="user-service",
                            imageName="runpod/tetra-rp-lb:latest")

# Deploy the endpoint (generates endpoint_url automatically)
await api.deploy()
# After deploy, api.endpoint_url is populated by RunPod
# Example: "https://xxx-yyy-zzz.runpod.io"

@remote(api, method="POST", path="/api/process")
async def process_data(x: int, y: int):
    return {"result": x + y}

# Call the function locally
result = await process_data(5, 3)

# What happens:
# 1. Decorator finds LoadBalancerSlsStub in registry
# 2. Stub extracts function source code via AST
# 3. Stub serializes arguments: cloudpickle.dumps([5, 3])
# 4. Stub POST to https://my-endpoint.runpod.ai/execute
# 5. Container receives request at /execute endpoint
# 6. create_lb_handler's execute_remote_function handles it:
#    a. Parses JSON body
#    b. Deserializes arguments: [5, 3]
#    c. Executes: exec(function_code) in isolated namespace
#    d. Calls func(5, 3)
#    e. Gets result: {"result": 8}
#    f. Serializes result via cloudpickle
#    g. Returns {success: true, result: base64_string}
# 7. Stub deserializes result
# 8. Returns {"result": 8} to caller
```

## Deployment Execution Model

### Local Development (LiveLoadBalancer)

When using `LiveLoadBalancer` for local testing, endpoints expose two types of routes:

1. **User-Defined Routes** (e.g., `/api/health`, `/api/users`)
   - Called via direct HTTP requests
   - Called via `@remote` decorator (uses /execute internally)

2. **Framework Endpoints**
   - `/execute` - Accepts serialized function code for @remote execution
   - `/ping` - Health check endpoint

### Deployed Endpoints (LoadBalancerSlsResource)

When deployed to production, endpoints **only expose user-defined routes** for security:

1. **User-Defined Routes** (e.g., `/api/health`, `/api/users`)
   - Called via direct HTTP requests from clients
   - Called via `@remote` decorator (stub translates to HTTP requests to user routes)
   - `/execute` endpoint NOT exposed (removed for security)

2. **Framework Endpoints**
   - `/ping` - Health check endpoint only

### Request Handling by Execution Type

#### Direct HTTP Requests (Always Works)

```
GET  /health
POST /api/users
PUT  /api/users/{user_id}
DELETE /api/users/{user_id}
```

**Characteristics:**
- Called by external HTTP clients
- FastAPI handles routing automatically
- Standard HTTP request/response
- No serialization/deserialization
- Direct function execution
- Errors return HTTP error codes

**Example:**
```python
@remote(api, method="GET", path="/health")
def health_check():
    return {"status": "ok"}

# Client can call:
GET https://my-endpoint.runpod.ai/health
# Response: 200 OK {"status": "ok"}
```

#### @remote Function Calls (Different Local vs Deployed)

**Local (LiveLoadBalancer):**
```python
@remote(api, method="POST", path="/api/process")
async def process_data(x: int, y: int):
    return {"result": x + y}

# Called via @remote:
result = await process_data(5, 3)  # Uses /execute internally (local only)
```

**Deployed (LoadBalancerSlsResource):**
```python
@remote(api, method="POST", path="/api/process")
async def process_data(x: int, y: int):
    return {"result": x + y}

# Called via @remote:
result = await process_data(5, 3)
# Stub automatically translates to: POST /api/process {"x": 5, "y": 3}
# No /execute endpoint involved (security)
```

**Key Differences:**
- Local: Serializes function code, POSTs to /execute
- Deployed: Maps arguments to JSON, POSTs to user-defined route
- No code changes needed - stub handles both automatically

**Important Implementation Detail: Stub Decision Logic**

The stub determines which execution path to use by checking:
1. Is this a `LiveLoadBalancer`? → Always use `/execute` for local development
2. Does the function have `method` and `path` metadata from `@remote` decorator? → If yes, use user-defined route
3. If routing metadata is incomplete or missing → Falls back to `/execute` (will fail on deployed endpoints)

This means if you decorate a function for `LoadBalancerSlsResource` without specifying both `method` and `path`, the stub will attempt to use `/execute`, which doesn't exist in production. Always provide complete routing metadata for deployed endpoints.

**Important Implementation Detail: Parameter Mapping**

When using user-defined routes (deployed endpoints), the stub inspects the function signature and maps positional and keyword arguments to the HTTP request JSON body:

```python
@remote(api, method="POST", path="/api/process")
async def process_data(x: int, y: int):
    return {"result": x + y}

# Local call:
result = await process_data(5, 3)

# Gets translated to:
POST /api/process
{
  "x": 5,
  "y": 3
}
```

The stub uses Python's `inspect.signature()` to map positional args to parameter names. This requires that:
- Function parameters are JSON-serializable types (int, str, bool, list, dict, None)
- Function signature is available (defined at module level, not dynamically created)
- No complex types (custom classes, Request objects, etc.) are used as parameters

## Execution Flow Diagram

```mermaid
graph TD
    A["HTTP Request arrives at<br/>RunPod Endpoint"] -->|HTTPS| B["RunPod Router<br/>Domain stripping"]
    B -->|Strips domain<br/>Forwards to container| C["Container Port 8000<br/>uvicorn/FastAPI"]

    C -->|Route decision| D{Is it /execute?}

    D -->|Yes: Framework| E["Framework Handler<br/>execute_remote_function"]
    D -->|No: User Route| F["FastAPI Router<br/>Match method + path in<br/>ROUTE_REGISTRY"]

    E --> E1["1. Parse JSON body"]
    E1 --> E2["2. Deserialize args/kwargs<br/>base64 + cloudpickle"]
    E2 --> E3["3. exec function_code<br/>in isolated namespace"]
    E3 --> E4["4. Call func with args"]
    E4 --> E5["5. Serialize result<br/>cloudpickle + base64"]
    E5 --> G["Build Response<br/>success: true/false"]

    F --> F1["1. Find handler function<br/>from ROUTE_REGISTRY"]
    F1 --> F2["2. Parse request parameters"]
    F2 --> F3["3. Call function<br/>with parameters"]
    F3 --> F4["4. Get result"]
    F4 --> G

    G -->|Serialize response| H["FastAPI Response Obj<br/>JSON or {success, result}"]
    H -->|Wrap in HTTPS| I["RunPod Router<br/>Wraps response"]
    I -->|Send back| J["HTTP Response to Client"]

    style A fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style B fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style C fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style D fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style E fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style F fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style E1 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style E2 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style E3 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style E4 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style E5 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style F1 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style F2 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style F3 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style F4 fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style G fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style H fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style I fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style J fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
```

## Security Model at Runtime

### /execute Endpoint

The `/execute` endpoint is an internal framework endpoint that:

1. **Accepts arbitrary Python code** (serialized as string)
2. **Executes it** in an isolated namespace using Python's `exec()`
3. **Returns results** back to caller

**Critical Security Model:**

The `/execute` endpoint is **only exposed on `LiveLoadBalancer` for local development**. It is **explicitly removed from deployed `LoadBalancerSlsResource` endpoints** for security reasons.

**Why This Design Is Necessary:**

The `/execute` endpoint accepts and executes arbitrary Python code sent in HTTP requests. An unauthorized user with access to this endpoint could:
- Execute system commands (e.g., `os.system()`)
- Access file system data (e.g., read environment variables, credentials)
- Modify application state or data
- Use your infrastructure for malicious purposes

**Why This Is Secure When Used Correctly:**

- In `LiveLoadBalancer` (local development): Code originates from your own `@remote` decorator
- You control what function code is serialized and sent
- Only accessible during local testing, never exposed publicly
- Same trusted-client model as queue-based serverless endpoints

**What Happens When Deployed:**

```
LiveLoadBalancer (local):
- /execute endpoint: INCLUDED (for @remote function execution)
- User routes: Included
- Safe because: Only you can run your code locally

LoadBalancerSlsResource (deployed):
- /execute endpoint: REMOVED for security
- User routes: Included
- Safe because: No arbitrary code execution possible
```

**If /execute Was Exposed (Don't Do This):**

```python
# Attacker's request
POST https://my-endpoint.runpod.ai/execute
{
  "function_name": "malicious",
  "function_code": "import os; os.system('rm -rf /')",
  "args": [],
  "kwargs": {}
}

# This would execute arbitrary system commands on your infrastructure
```

**Best Practices:**

- Never manually add `/execute` to deployed endpoints
- Use the default `create_lb_handler()` behavior (removes `/execute`)
- Always use `LoadBalancerSlsResource` for production (not `LiveLoadBalancer`)
- Test locally with `LiveLoadBalancer` first
- For debugging deployed endpoints, use container logs, not code injection

## Concurrency and Scaling

### How RunPod Handles Concurrent Requests

```mermaid
graph TD
    A["Request 1<br/>POST /api/process"] -->|→ Worker 1| B["Container [Worker 1]<br/>Executes Request 1"]
    C["Request 2<br/>POST /api/users"] -->|→ Worker 1| D["Queued in Worker 1"]
    D -->|Worker available| E["Container [Worker 1]<br/>Executes Request 2<br/>Concurrently"]
    F["Request 3<br/>POST /api/health"] -->|→ Worker 2| G["Container [Worker 2]<br/>Executes Request 3"]

    H["RunPod Scaler<br/>REQUEST_COUNT"] -->|Queue grows| I["Monitor Queue Depth"]
    I -->|Q ≥ 3| J["Spin up Worker 3"]
    I -->|Q ≥ 6| K["Spin up Worker 4"]
    I -->|Q empty| L["Wind down Workers"]

    style A fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style B fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style C fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style D fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style E fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style F fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff
    style G fill:#0d7f1f,stroke:#0d4f1f,stroke-width:3px,color:#fff
    style H fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style I fill:#ff6b35,stroke:#c41e0f,stroke-width:3px,color:#fff
    style J fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style K fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
    style L fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff
```

### Function Execution

- Each request executes in isolated context
- async functions execute with asyncio
- Multiple requests can process concurrently (with async)
- Synchronous functions block worker thread

**Example Concurrency:**

```python
@remote(api, method="POST", path="/api/process")
async def process_data(x: int):
    import time
    await asyncio.sleep(10)  # Simulate work
    return {"result": x}

# If 5 requests come in simultaneously:
# - Request 1: await asyncio.sleep(10) → Worker 1
# - Request 2: await asyncio.sleep(10) → Worker 1 (concurrent)
# - Request 3: await asyncio.sleep(10) → Worker 1 (concurrent)
# - Request 4: await asyncio.sleep(10) → Worker 2 (new worker)
# - Request 5: await asyncio.sleep(10) → Worker 2 (concurrent)
#
# All 5 complete in ~10s (concurrent within workers)
```

## Error Handling at Runtime

### Client Errors

```
POST https://endpoint.runpod.ai/api/users
{"invalid": "json"

# Response: 422 Unprocessable Entity
{
  "detail": [
    {
      "type": "json_error",
      "loc": ["body"],
      "msg": "JSON decode error"
    }
  ]
}
```

### Function Errors

```
@remote(api, method="POST", path="/api/users")
async def create_user(name: str):
    if not name:
        raise ValueError("Name required")
    return {"id": 1, "name": name}

# Call with invalid data:
POST https://endpoint.runpod.ai/api/users
{"name": ""}

# Response: 422 Validation Error or 500 Internal Error
# (depending on where error occurs)
```

### @remote Execution Errors

```python
# Local code
@remote(api, method="POST", path="/api/process")
async def process_data(x: int):
    raise RuntimeError("Processing failed")

result = await process_data(5)
# Raises RuntimeError: "Remote execution failed: Processing failed"
```

## Performance Characteristics

### Request Latency (approximate)

```
Direct HTTP Request:
- Request → RunPod Router: 10-50ms
- FastAPI routing: 1-5ms
- Function execution: Variable
- Serialization: Variable
- Response: 10-50ms
Total (no-op function): 30-110ms

@remote Function Call:
- Function serialization: 1-10ms
- HTTP request to /execute: 10-50ms
- Deserialization: 1-10ms
- Function execution: Variable
- Result serialization: 1-10ms
- Result deserialization: 1-10ms
- Response: 10-50ms
Total (no-op function): 40-150ms
```

### Memory Usage

- FastAPI app baseline: ~50-100MB
- Per function in namespace: ~0.5-5MB
- Serialized args/result: Variable (depends on data size)
- RunPod allocates: Depends on pod type

### Request Size Limits

- RunPod has limits on request body size
- Serialized data (via cloudpickle) increases size
- Large arguments may hit limits
- Consider streaming for large payloads

## Monitoring and Debugging at Runtime

### Logs Available on RunPod

```
Container logs (uvicorn/FastAPI):
- Request arrival
- Route matching
- Function execution
- Errors and exceptions
- Response generation

Environment:
- Pod ID
- Worker ID
- GPU allocation
- Memory usage
```

### Health Checks

```
GET https://endpoint.runpod.ai/ping
Response: 200 OK {"status": "healthy"}

RunPod polls /ping every 30 seconds
- 200 OK → Worker healthy
- Non-200 → Worker unhealthy
- No response → Worker down
- Unhealthy workers replaced
```

### Common Issues at Runtime

**"Connection refused"**
- Container not running
- Uvicorn failed to start
- Check container logs

**"Timeout after 30s"**
- Function took >30s
- Network issue
- Increase timeout if needed

**"500 Internal Server Error"**
- Function raised exception
- Check container logs
- Verify function code

## Deployment Considerations

### Image Selection

```
tetra-rp-lb:latest (default)
- FastAPI + uvicorn pre-installed
- Tetra runtime dependencies
- Optimized for LB endpoints

Custom image:
- Must have FastAPI, uvicorn
- Must expose port 8000
- /ping endpoint should work
```

### Pod Configuration

```python
LoadBalancerSlsResource(
    name="my-api",
    imageName="runpod/tetra-rp-lb:latest",
    gpus=[GpuGroup.AMPERE_80],      # Optional: if compute needed
    instanceIds=[...],               # Or specify CPU instances
    workersMax=5,                    # Max concurrent workers
    template=PodTemplate(...)        # Storage, env vars, etc.
)
```

### Network

```
Incoming:
- HTTPS endpoint provided by RunPod
- Auto-scaled based on REQUEST_COUNT
- Health checks ensure availability

Outgoing:
- Your functions can make HTTP requests
- Can access external APIs
- Can access other RunPod endpoints
```

## Summary

**What Happens at Runtime:**

1. **Deployment** - FastAPI app runs in RunPod container
2. **Request Arrival** - HTTP request reaches container
3. **Routing** - FastAPI matches method/path to function
4. **Execution** - Function code runs with parameters
5. **Response** - Result serialized and returned

**Two Execution Paths:**

- **User Routes** - Direct HTTP from clients
- **Framework Routes** - @remote calls from local code via /execute

**Key Characteristics:**

- ✅ Low latency (direct HTTP)
- ✅ No queuing overhead
- ✅ Concurrent request handling
- ✅ FastAPI routing
- ✅ Serialized function execution via @remote

**Security:**

- Protect `/execute` endpoint with authentication
- Only allow @remote calls from trusted sources
- Monitor endpoint usage
